#!/usr/bin/env python3
"""
Usage: .venv/bin/python -m tests.memory_buckets_benchmark [--device cuda] [--model koharune-ami] [--iterations 30]

ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒã‚±ãƒ„åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import gc
import time
from pathlib import Path

import numpy as np
import torch
from numpy.typing import NDArray
from scipy.io import wavfile

from style_bert_vits2.constants import (
    BASE_DIR,
    DEFAULT_LENGTH,
    DEFAULT_NOISE,
    DEFAULT_NOISEW,
    DEFAULT_SDP_RATIO,
    Languages,
)
from style_bert_vits2.logging import logger
from style_bert_vits2.models.infer import infer
from style_bert_vits2.models.memory_efficient import (
    clear_memory_pools,
    get_memory_pool_stats,
)
from style_bert_vits2.nlp import bert_models
from style_bert_vits2.tts_model import TTSModel, TTSModelHolder


def save_audio_file(
    audio_data: NDArray[np.float32], sample_rate: int, text: str, output_type: str
) -> None:
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚"""
    try:
        output_dir = Path("tests/wavs/memory_buckets_benchmark")
        output_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’ç½®æ›
        safe_filename = (
            text.replace("/", "_")
            .replace("\\", "_")
            .replace(":", "")
            .replace("*", "")
            .replace("?", "")
            .replace("<", "")
            .replace(">", "")
            .replace("|", "")
            .replace('"', "")
            .replace("!", "")
        )

        # ãƒ•ã‚¡ã‚¤ãƒ«åãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
        max_filename_length = 50  # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åé•·åˆ¶é™
        if len(safe_filename) > max_filename_length:
            safe_filename = safe_filename[:max_filename_length] + "..."

        output_path = output_dir / f"{safe_filename}_{output_type}.wav"

        # 16bitæ•´æ•°ã«å¤‰æ›ã—ã¦WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        audio_int16 = (audio_data * 32767).astype(np.int16)
        wavfile.write(str(output_path), sample_rate, audio_int16)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æˆåŠŸã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.debug(f"Audio saved: {output_path}")

    except ImportError:
        logger.warning("scipy is required to save audio files, skipping audio save")
    except Exception as ex:
        logger.error(f"Failed to save audio file: {ex}")


# ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ§˜ã€…ãªé•·ã•ï¼‰
TEST_TEXTS = [
    "ã“ã‚“ã«ã¡ã¯ã€‚",  # çŸ­ã„
    "ä»Šæ—¥ã¯ã¨ã¦ã‚‚ã„ã„å¤©æ°—ã§ã™ã­ã€‚",  # ä¸­ç¨‹åº¦
    "æ˜¨æ—¥ã¯é›¨ãŒé™ã£ã¦ã„ã¾ã—ãŸãŒã€ä»Šæ—¥ã¯æ™´ã‚Œã¦ã¨ã¦ã‚‚æ°—æŒã¡ãŒã„ã„ã§ã™ã€‚",  # ã‚„ã‚„é•·ã„
    "äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Šã€éŸ³å£°åˆæˆæŠ€è¡“ã‚‚å¤§ããé€²æ­©ã—ã¾ã—ãŸã€‚ã‚ˆã‚Šè‡ªç„¶ã§è¡¨ç¾è±Šã‹ãªéŸ³å£°ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚",  # é•·ã„
    "æ˜¥ã®è¨ªã‚Œã¨ã¨ã‚‚ã«ã€æ¡œã®èŠ±ãŒå’²ãå§‹ã‚ã¾ã—ãŸã€‚æ·¡ã„ãƒ”ãƒ³ã‚¯è‰²ã®èŠ±ã³ã‚‰ãŒé¢¨ã«èˆã„ã€è¡—å…¨ä½“ãŒè¯ã‚„ã‹ãªé›°å›²æ°—ã«åŒ…ã¾ã‚Œã¦ã„ã¾ã™ã€‚å¤šãã®äººã€…ãŒèŠ±è¦‹ã‚’æ¥½ã—ã¿ã€ç¬‘é¡”ãŒã‚ãµã‚Œã‚‹å­£ç¯€ã¨ãªã‚Šã¾ã—ãŸã€‚",  # ã¨ã¦ã‚‚é•·ã„
]


class MemoryTracker:
    """GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¿½è·¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, device: str):
        self.device = device
        self.snapshots: list[dict[str, float]] = []

    def snapshot(self, label: str):
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
        if self.device == "cuda" and torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3  # GB
            fragmentation = reserved - allocated
        else:
            allocated = reserved = fragmentation = 0.0

        self.snapshots.append(
            {
                "label": label,  # type: ignore
                "allocated_gb": allocated,
                "reserved_gb": reserved,
                "fragmentation_gb": fragmentation,
            }
        )

    def print_report(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        print("\n=== Memory Usage Report ===")
        print(
            f"{'Label':<30} {'Allocated (GB)':>15} {'Reserved (GB)':>15} {'Fragmentation (GB)':>20}"
        )
        print("-" * 82)

        for snap in self.snapshots:
            print(
                f"{snap['label']:<30} {snap['allocated_gb']:>15.3f} "
                f"{snap['reserved_gb']:>15.3f} {snap['fragmentation_gb']:>20.3f}"
            )


def measure_inference_performance(
    model: TTSModel,
    text: str,
    device: str,
    use_buckets: bool,
    use_fp16: bool,
    save_audio: bool = False,
) -> tuple[float, float, NDArray[np.float32]]:
    """
    æ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šã™ã‚‹

    Args:
        save_audio: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹

    Returns:
        tuple: (æ¨è«–æ™‚é–“, ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡MB, éŸ³å£°ãƒ‡ãƒ¼ã‚¿)
    """
    # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()

    start_time = time.perf_counter()

    net_g = model.net_g
    assert net_g is not None

    style_vec = model.get_style_vector(0, 1.0)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚¿ã‚¤ãƒ«

    # æ¨è«–ã‚’å®Ÿè¡Œ
    with torch.inference_mode():
        audio_data = infer(
            text=text,
            style_vec=style_vec,
            sdp_ratio=DEFAULT_SDP_RATIO,
            noise_scale=DEFAULT_NOISE,
            noise_scale_w=DEFAULT_NOISEW,
            length_scale=DEFAULT_LENGTH,
            sid=0,
            language=Languages.JP,
            hps=model.hyper_parameters,
            net_g=net_g,
            device=device,
            use_fp16=use_fp16,
            clear_cuda_cache=False,  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¸­ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã‚’ç„¡åŠ¹åŒ–
            use_memory_efficient_buckets=use_buckets,
            model_name="benchmark",
        )

    end_time = time.perf_counter()
    inference_time = end_time - start_time

    # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    if device == "cuda":
        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB
    else:
        peak_memory = 0.0

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if save_audio:
        output_type = "with_buckets" if use_buckets else "without_buckets"
        save_audio_file(
            audio_data, model.hyper_parameters.data.sampling_rate, text, output_type
        )

    return inference_time, peak_memory, audio_data


def run_benchmark(
    device: str = "cuda",
    model_name: str = "koharune-ami",
    num_iterations: int = 30,
    use_fp16: bool = True,
) -> None:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹"""

    print("=" * 80)
    print("Style-Bert-VITS2 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒã‚±ãƒ„åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 80)
    print(f"ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model_name}")
    print(f"åå¾©å›æ•°: {num_iterations}")
    print(f"FP16: {use_fp16}")
    print("=" * 80)

    # BERT ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š
    bert_memory_usage = 0.0
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()

        memory_before_bert = torch.cuda.memory_allocated() / (1024 * 1024)
        logger.info("Loading BERT model for memory measurement...")
        bert_models.load_model(Languages.JP, device_map=device, use_fp16=use_fp16)
        memory_after_bert = torch.cuda.memory_allocated() / (1024 * 1024)
        bert_memory_usage = memory_after_bert - memory_before_bert

        print(f"BERT ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {bert_memory_usage:.2f}MB")
        print("=" * 80)

    # ãƒ¢ãƒ‡ãƒ«ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
    model_holder = TTSModelHolder(
        BASE_DIR / "model_assets",
        device,
        onnx_providers=[],
        ignore_onnx=True,
        use_fp16=use_fp16,
    )

    if len(model_holder.models_info) == 0:
        print("ã‚¨ãƒ©ãƒ¼: éŸ³å£°åˆæˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    model_info = None
    for info in model_holder.models_info:
        if info.name == model_name:
            model_info = info
            break

    if model_info is None:
        print(f'ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ« "{model_name}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')
        print("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
        for info in model_holder.models_info:
            print(f"  - {info.name}")
        return

    # Safetensors å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    model_files = [
        f
        for f in model_info.files
        if f.endswith(".safetensors") and not f.startswith(".")
    ]
    if len(model_files) == 0:
        print(
            f'ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ« "{model_name}" ã® .safetensors ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚'
        )
        return

    model_file = model_files[0]
    print(f"ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_file}")
    print()

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    model = model_holder.get_model(model_name, model_file)
    model.load()

    # ãƒã‚±ãƒ„åŒ–ãªã—ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\n" + "=" * 60)
    print("ãƒã‚±ãƒ„åŒ–ãªã—ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
    print("=" * 60)

    tracker_without = MemoryTracker(device)
    tracker_without.snapshot("Initial")

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for i in range(2):
        _, _, _ = measure_inference_performance(
            model, TEST_TEXTS[0], device, use_buckets=False, use_fp16=use_fp16
        )

    torch.cuda.empty_cache()
    gc.collect()
    tracker_without.snapshot("After warmup")

    # æœ¬æ¸¬å®š
    inference_times_without = []
    for i in range(num_iterations):
        text = TEST_TEXTS[i % len(TEST_TEXTS)]
        # æœ€å¾Œã®æ•°å›ã®ã¿éŸ³å£°ã‚’ä¿å­˜ï¼ˆä¸»è¦³è©•ä¾¡ç”¨ï¼‰
        save_audio = i >= max(0, num_iterations - len(TEST_TEXTS))
        inference_time, peak_memory, _ = measure_inference_performance(
            model,
            text,
            device,
            use_buckets=False,
            use_fp16=use_fp16,
            save_audio=save_audio,
        )
        inference_times_without.append(inference_time)

        if (i + 1) % 10 == 0:
            tracker_without.snapshot(f"After {i + 1} iterations")
            print(f"  åå¾© {i + 1}/{num_iterations} å®Œäº†")

    tracker_without.snapshot("Final (before cleanup)")

    # ãƒã‚±ãƒ„åŒ–ã‚ã‚Šã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\n" + "=" * 60)
    print("ãƒã‚±ãƒ„åŒ–ã‚ã‚Šã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
    print("=" * 60)

    tracker_with = MemoryTracker(device)
    tracker_with.snapshot("Initial")

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for i in range(2):
        _, _, _ = measure_inference_performance(
            model, TEST_TEXTS[0], device, use_buckets=True, use_fp16=use_fp16
        )

    torch.cuda.empty_cache()
    gc.collect()
    tracker_with.snapshot("After warmup")

    # æœ¬æ¸¬å®š
    inference_times_with = []
    for i in range(num_iterations):
        text = TEST_TEXTS[i % len(TEST_TEXTS)]
        # æœ€å¾Œã®æ•°å›ã®ã¿éŸ³å£°ã‚’ä¿å­˜ï¼ˆä¸»è¦³è©•ä¾¡ç”¨ï¼‰
        save_audio = i >= max(0, num_iterations - len(TEST_TEXTS))
        inference_time, peak_memory, _ = measure_inference_performance(
            model,
            text,
            device,
            use_buckets=True,
            use_fp16=use_fp16,
            save_audio=save_audio,
        )
        inference_times_with.append(inference_time)

        if (i + 1) % 10 == 0:
            tracker_with.snapshot(f"After {i + 1} iterations")
            print(f"  åå¾© {i + 1}/{num_iterations} å®Œäº†")

    tracker_with.snapshot("Final (before cleanup)")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    clear_memory_pools("benchmark")
    torch.cuda.empty_cache()
    gc.collect()
    tracker_with.snapshot("After cleanup")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
    model.unload()

    # çµæœæ¯”è¼ƒ
    print("\n" + "=" * 60)
    print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœæ¯”è¼ƒ")
    print("=" * 60)

    # æ¨è«–æ™‚é–“æ¯”è¼ƒ
    avg_without = np.mean(inference_times_without)
    avg_with = np.mean(inference_times_with)
    overhead = (avg_with / avg_without - 1) * 100

    print("\næ¨è«–æ™‚é–“:")
    print(
        f"  ãƒã‚±ãƒ„åŒ–ãªã—: {avg_without:.3f} s (æ¨™æº–åå·®: {np.std(inference_times_without):.3f})"
    )
    print(
        f"  ãƒã‚±ãƒ„åŒ–ã‚ã‚Š: {avg_with:.3f} s (æ¨™æº–åå·®: {np.std(inference_times_with):.3f})"
    )
    print(f"  ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: {overhead:+.1f}%")

    # ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–æ¯”è¼ƒ
    final_without = next(
        s for s in tracker_without.snapshots if s["label"] == "Final (before cleanup)"
    )
    final_with = next(
        s for s in tracker_with.snapshots if s["label"] == "Final (before cleanup)"
    )

    print("\nãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–:")
    print(f"  ãƒã‚±ãƒ„åŒ–ãªã—: {final_without['fragmentation_gb']:.3f} GB")
    print(f"  ãƒã‚±ãƒ„åŒ–ã‚ã‚Š: {final_with['fragmentation_gb']:.3f} GB")
    print(
        f"  å‰Šæ¸›é‡: {final_without['fragmentation_gb'] - final_with['fragmentation_gb']:.3f} GB"
    )

    # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
    pool_stats = get_memory_pool_stats()
    if pool_stats:
        print("\nãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«çµ±è¨ˆ:")
        for pool_name, stats in pool_stats.items():
            print(f"  {pool_name}: {stats}")

    # è©³ç´°ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ
    print("\n=== ãƒã‚±ãƒ„åŒ–ãªã— - ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ ===")
    tracker_without.print_report()

    print("\n=== ãƒã‚±ãƒ„åŒ–ã‚ã‚Š - ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ ===")
    tracker_with.print_report()

    # åˆ¤å®š
    print("\n=== ç·åˆè©•ä¾¡ ===")
    if overhead < 5:
        print("âœ“ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ (< 5%)")
    elif overhead < 15:
        print("âš  è»½å¾®ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ (5-15%)")
    else:
        print("âœ— å¤§ããªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ (> 15%)")

    fragmentation_reduction = (
        final_without["fragmentation_gb"] - final_with["fragmentation_gb"]
    )
    if fragmentation_reduction > 0.1:
        print("âœ“ ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸ")
    elif fragmentation_reduction > 0.05:
        print("âœ“ ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–ãŒæ”¹å–„ã•ã‚Œã¾ã—ãŸ")
    else:
        print("âš  ãƒ¡ãƒ¢ãƒªæ–­ç‰‡åŒ–ã®æ”¹å–„ã¯é™å®šçš„ã§ã™")

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã®é€šçŸ¥
    audio_dir = Path("tests/wavs/memory_buckets_benchmark")
    if audio_dir.exists() and any(audio_dir.iterdir()):
        print(f"\nğŸ§ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {audio_dir}")
        print("   - *_without_buckets.wav: ãƒã‚±ãƒ„åŒ–ãªã—ã®éŸ³å£°")
        print("   - *_with_buckets.wav: ãƒã‚±ãƒ„åŒ–ã‚ã‚Šã®éŸ³å£°")
        print("   ä¸»è¦³è©•ä¾¡ã§éŸ³è³ªã®æ¯”è¼ƒã‚’ã—ã¦ãã ã•ã„ã€‚")


def main():
    parser = argparse.ArgumentParser(description="Memory-efficient buckets benchmark")
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cpu", "cuda"],
        help="Device to use (default: cuda)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="koharune-ami",
        help="Model name to use (default: koharune-ami)",
    )
    parser.add_argument(
        "--iterations", type=int, default=30, help="Number of iterations (default: 30)"
    )
    parser.add_argument(
        "--fp16",
        dest="use_fp16",
        action="store_true",
        help="Use FP16 inference (default)",
    )
    parser.add_argument(
        "--no-fp16",
        dest="use_fp16",
        action="store_false",
        help="Disable FP16 inference",
    )
    parser.set_defaults(use_fp16=True)

    args = parser.parse_args()

    try:
        run_benchmark(
            device=args.device,
            model_name=args.model,
            num_iterations=args.iterations,
            use_fp16=args.use_fp16,
        )
    except KeyboardInterrupt:
        print("\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as ex:
        logger.exception(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ex}")


if __name__ == "__main__":
    main()
